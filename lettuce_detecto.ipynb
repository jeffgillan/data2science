{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lettuce Detection with Detecto\n",
    "\n",
    "This notebook is a example of how to use the Detecto python library to detect lettuce in aerial drone images. The Detecto library is a PyTorch-based library that simplifies the process of training object detection models and then predicting the objects in imagery. \n",
    "\n",
    "This notebook accomplishes the following tasks:\n",
    "1. Connects to Data to Science platform (https://ps2.d2s.org) to access the drone imagery. The dataset is an orthomosaic of a lettuce field near Yuma, Arizona.\n",
    "2. Downloads (from Huggingface) an object detection machine learning model that has been fine-tuned to detect lettuce. The model is based on the Faster R-CNN architecture. It was trained by PhytoOracle, a research group at the University of Arizona. It was trained on Maricopa Ag Center gantry images at very high-resolution (millimeters). It is trained to use RGB images and put bounding boxes around lettuce plants.\n",
    "3. Outputs the detected lettuce bounding boxes as a polygon shapefile layer with the same geographic coordinate system as the input drone image. \n",
    "4. Demonstrates how to fine-tune (training) the model on a small dataset of annotated lettuce images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries for D2S and leafmap\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from d2spy.workspace import Workspace\n",
    "\n",
    "import os\n",
    "\n",
    "import leafmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules from Detecto python library\n",
    "from detecto.core import Model #bring in the Faster R-CNN ResNet50 FPM model\n",
    "from detecto.utils import read_image\n",
    "from detecto.visualize import show_labeled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import more useful libraries\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Purdue hosted D2S instance. It will prompt for your D2S password.\n",
    "workspace = Workspace.connect(\"https://ps2.d2s.org\", \"jgillan@arizona.edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all your projects in D2S\n",
    "projects = workspace.get_projects()\n",
    "\n",
    "# Check if there are any projects\n",
    "if len(projects) > 0:\n",
    "    # Loop through all projects and print each one\n",
    "    for project in projects:\n",
    "        print(f\"ID: {project.id}\")\n",
    "        print(f\"Title: {project.title}\")\n",
    "        print(f\"Description: {project.description}\\n\")\n",
    "else:\n",
    "    print(\"Please create a project before proceeding with this guide.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Choose a project (from the previous print) and list all flights for that project\n",
    "\n",
    "# Define the project ID you're looking for\n",
    "#project_id = \"46669dd1-3c9a-487e-adaa-92dd50bf0420\"\n",
    "project_id = \"cfb77a4a-a065-400e-ae12-2cf11bf7c25b\"\n",
    "\n",
    "\n",
    "# Find the project by ID\n",
    "selected_project = None\n",
    "for project in projects:\n",
    "    if project.id == project_id:\n",
    "        selected_project = project\n",
    "        break\n",
    "\n",
    "# Check if the project was found\n",
    "if selected_project:\n",
    "    # Get list of all flights for the selected project\n",
    "    flights = selected_project.get_flights()\n",
    "\n",
    "    # Check if there are any flights\n",
    "    if len(flights) > 0:\n",
    "        # Loop through all flights and print each one\n",
    "        for flight in flights:\n",
    "            print(flight)\n",
    "    else:\n",
    "        print(\"No flights found for this project.\")\n",
    "else:\n",
    "    print(f\"Project with ID '{project_id}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of data products from a flight. O in this case is the first flight listed.\n",
    "data_products = flights[0].get_data_products()\n",
    "\n",
    "# Check if there are any data products\n",
    "if len(data_products) > 0:\n",
    "    # Loop through all data products and print their URLs\n",
    "    for product in data_products:\n",
    "        print(product.url)\n",
    "else:\n",
    "    print(\"No data products found for this flight.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display Interactive leafmap Map\n",
    "\n",
    "# Set the TITILER_ENDPOINT environment variable to the D2S hosted Titiler endpoint. Titiler is a cloud optimized GeoTIFF tile server.\n",
    "os.environ[\"TITILER_ENDPOINT\"] = \"https://tt.d2s.org\"\n",
    "\n",
    "m = leafmap.Map()\n",
    "\n",
    "# URL for a D2S hosted GeoTIFF data product\n",
    "ortho_url = \"https://ps2.d2s.org/static/projects/cfb77a4a-a065-400e-ae12-2cf11bf7c25b/flights/20812f2d-4b31-45b0-9b2f-66595aac16fa/data_products/ed454526-6e85-49f8-a274-fafda388abdd/b1cdf543-0dfd-4da7-b258-edb2fad2f5bd.tif\"\n",
    "\n",
    "# Add a publicly available data product to the map\n",
    "m.add_cog_layer(ortho_url, name=\"Orthomosaic\")\n",
    "\n",
    "# If you want to display a private data product, comment out the previously line and uncomment the below m.add_cog_layer line\n",
    "# Add a private data product to the map\n",
    "# m.add_cog_layer(f\"{ortho_url}?API_KEY={api_key}\", name=\"DSM\", colormap_name=\"rainbow\")\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the image from D2S to the local directory\n",
    "#!wget https://ps2.d2s.org/static/projects/46669dd1-3c9a-487e-adaa-92dd50bf0420/flights/72e1b4a1-68ea-48bf-85d1-4f6315cd78bd/data_products/fcb09a00-181c-4e3c-ba89-e58c7e7a7223/3ac72e63-64fe-4713-af0b-c332b3851032.tif\n",
    "\n",
    "#!wget https://ps2.d2s.org/static/projects/cfb77a4a-a065-400e-ae12-2cf11bf7c25b/flights/20812f2d-4b31-45b0-9b2f-66595aac16fa/data_products/ed454526-6e85-49f8-a274-fafda388abdd/b1cdf543-0dfd-4da7-b258-edb2fad2f5bd.tif\n",
    "\n",
    "#Set the image path as a variable \n",
    "image_path = \"b1cdf543-0dfd-4da7-b258-edb2fad2f5bd.tif\"\n",
    "\n",
    "# Disable the image size limit of PIL\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "image = Image.open(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the image\n",
    "original_width, original_height = image.size\n",
    "print(f\"Original Image Size: {original_width}x{original_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the fine-tuned model (lettuce detection) from Hugging Face\n",
    "!wget https://huggingface.co/jgillan/phytooracle_lettuce_2/resolve/main/model_weights.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the labels for the fine-tuned model\n",
    "labels = [\n",
    "    'lettuce'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the fine-tuned model\n",
    "model = Model.load('model_weights.pth', labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code are a series of functions to:\n",
    "1. Crop the drone aerial image into smaller tiles. This is necessary because the faster r-cnn model wants to process images of a certain size. By default, the maximum size of the long side of an image is around 1333 pixels.The drone orthomosasics are much too big (e.g., 13569x39850). The faster r-cnn default image size can be adjusted, but large images can cause memory issues and long processing times. If you try to use faster r-cnn on an image larger than the default, it will automatically downsample the image to the default size (e.g., 480x1344). This can cause the model to miss small objects. The better solution is to crop the image into smaller tiles and process each tile separately.\n",
    "\n",
    "2. Detect lettuce in the cropped images. The detecto library has a function that takes an image and returns the bounding boxes of the detected objects. The function returns the bounding boxes in the format (x_min, y_min, x_max, y_max). The function also returns the confidence of the detection. The confidence is a number between 0 and 1. The higher the number, the more confident the model is that the object is a lettuce plant. The function also returns the class of the detected object. The class is a string that tells you what type of object was detected. In this case, the class is always \"lettuce\".\n",
    "\n",
    "3. Display the predicted bounding boxes on the original large image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define a series of functions: \n",
    "\n",
    "\n",
    "# Step 1: Crop the large image into jpeg tiles\n",
    "def crop_image_into_tiles(image_path, tile_size, output_folder):\n",
    "    image = Image.open(image_path)\n",
    "    img_width, img_height = image.size\n",
    "    tiles_with_coordinates = []\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    tile_num = 0\n",
    "    for y in range(0, img_height, tile_size):\n",
    "        for x in range(0, img_width, tile_size):\n",
    "            box = (x, y, min(x + tile_size, img_width), min(y + tile_size, img_height))\n",
    "            tile = image.crop(box)\n",
    "\n",
    "            if tile.mode == 'RGBA':  # Convert to RGB if needed\n",
    "                tile = tile.convert(\"RGB\")\n",
    "\n",
    "            tile_path = os.path.join(output_folder, f'tile_{tile_num}.jpg')\n",
    "            tile.save(tile_path, 'JPEG')\n",
    "            tiles_with_coordinates.append((tile_path, (x, y)))\n",
    "            tile_num += 1\n",
    "\n",
    "    return tiles_with_coordinates\n",
    "\n",
    "# Step 2: Run object detection predictions on each tile\n",
    "def get_predictions_for_tiles(tiles_with_coordinates, model, threshold=0.3):\n",
    "    all_predictions = []\n",
    "\n",
    "    for tile_path, (tile_x, tile_y) in tiles_with_coordinates:\n",
    "        tile = Image.open(tile_path)\n",
    "        \n",
    "        # Run prediction on tile\n",
    "        labels, boxes, scores = model.predict(tile)\n",
    "        \n",
    "        # Store the predictions with the tile's coordinates\n",
    "        all_predictions.append((labels, boxes, scores, tile_x, tile_y))\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "# Step 3: Adjust and display predictions on the original image\n",
    "def adjust_boxes(boxes, tile_x, tile_y):\n",
    "    adjusted_boxes = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        adjusted_box = [x_min + tile_x, y_min + tile_y, x_max + tile_x, y_max + tile_y]\n",
    "        adjusted_boxes.append(adjusted_box)\n",
    "    return adjusted_boxes\n",
    "\n",
    "def display_original_image_with_boxes(image_path, all_predictions, threshold=0.3):\n",
    "    image = Image.open(image_path)\n",
    "    fig, ax = plt.subplots(1, figsize=(100, 100))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for labels, boxes, scores, tile_x, tile_y in all_predictions:\n",
    "        adjusted_boxes = adjust_boxes(boxes, tile_x, tile_y)\n",
    "\n",
    "        for i, box in enumerate(adjusted_boxes):\n",
    "            if scores[i] > threshold:\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=3, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                #label_text = f'{labels[i]}: {scores[i]:.2f}' # Uncomment these lines to display labels and scores\n",
    "                #plt.text(x_min, y_min - 10, label_text, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input some parameters and run the functions\n",
    "\n",
    "# Define the output folder for the cropped images and tile size\n",
    "output_folder = 'cropped_tiles'\n",
    "tile_size = 1024  # Adjust as needed\n",
    "\n",
    "# Run the tiling function\n",
    "tiles_with_coordinates = crop_image_into_tiles(image_path, tile_size, output_folder)\n",
    "\n",
    "### Run the prediction function on the tiles ### This will take some time to run\n",
    "all_predictions = get_predictions_for_tiles(tiles_with_coordinates, model, threshold=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all predictions on the original large image\n",
    "display_original_image_with_boxes(image_path, all_predictions, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to create a georeferenced shapefile from the predictions\n",
    "\n",
    "def create_georeferenced_shapefile(all_predictions, image_path, output_shapefile, threshold):\n",
    "    # Open the original image to retrieve CRS and affine transform\n",
    "    with rasterio.open(image_path) as src:\n",
    "        crs = src.crs  # CRS of the original image (EPSG:32611)\n",
    "        transform = src.transform  # Affine transformation matrix for the full image\n",
    "\n",
    "    # List to hold each bounding box with its label and score\n",
    "    polygons = []\n",
    "\n",
    "    # Collect bounding boxes in terms of full image coordinates (not tile-based)\n",
    "    for labels, boxes, scores, tile_x, tile_y in all_predictions:\n",
    "        for i, box_coords in enumerate(boxes):\n",
    "            if scores[i] >= threshold:  # Apply confidence threshold\n",
    "                # Calculate full-image coordinates for each bounding box\n",
    "                x_min, y_min, x_max, y_max = box_coords\n",
    "                x_min += tile_x\n",
    "                y_min += tile_y\n",
    "                x_max += tile_x\n",
    "                y_max += tile_y\n",
    "\n",
    "                # Create a Polygon in image (pixel) coordinates\n",
    "                polygon = box(x_min, y_min, x_max, y_max)\n",
    "                polygons.append({\n",
    "                    'geometry': polygon,\n",
    "                    'label': labels[i],\n",
    "                    'score': scores[i]\n",
    "                })\n",
    "\n",
    "    # Create a GeoDataFrame with pixel-based coordinates\n",
    "    gdf_pixel = gpd.GeoDataFrame(polygons, crs=\"EPSG:32611\")\n",
    "\n",
    "    # Apply affine transform to convert pixel coordinates to geographic coordinates\n",
    "    gdf_pixel['geometry'] = gdf_pixel['geometry'].apply(\n",
    "        lambda geom: transform_polygon(geom, transform)\n",
    "    )\n",
    "\n",
    "    # Set CRS and save to shapefile\n",
    "    gdf_pixel.set_crs(crs, inplace=True)\n",
    "    gdf_pixel.to_file(output_shapefile, driver=\"ESRI Shapefile\")\n",
    "\n",
    "def transform_polygon(geometry, transform):\n",
    "    # Convert each coordinate in the Polygon to geographic coordinates using affine transform\n",
    "    transformed_coords = [(transform * (x, y)) for x, y in geometry.exterior.coords]\n",
    "    return box(*transformed_coords[0], *transformed_coords[2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the function to create the georeferenced shapefile\n",
    "output_shapefile = \"predicted_please_work.shp\"\n",
    "create_georeferenced_shapefile(all_predictions, image_path, output_shapefile, threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes access token from future requests to D2S\n",
    "workspace.logout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune a model with training data\n",
    "\n",
    "The detecto library also has a function to fine-tune a model with a small dataset of annotated images. For image labeling, I would recommend an open-source program called [Label Studio](https://labelstud.io/). In Label Studio, I can bring in the tile images (non-georeferenced), draw bounding boxes around the lettuce plants, and export the annotations in several formats. The detecto library can read the annotations in the Pascal VOC format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from detecto.core import Dataset\n",
    "from detecto.core import DataLoader, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets from the training annotations and images\n",
    "dataset = Dataset(label_data='./lettuce_training/Annotations/', image_folder='./lettuce_training/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first training image and its bounding boxes\n",
    "image, target = dataset[0]\n",
    "show_labeled_image(image, target['boxes'], target['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# wrap the training set in a DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# specify all unique labels you're trying to predict\n",
    "labels = [\n",
    "    'lettuce'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning choices\n",
    "\n",
    " We are going to fine-tune train the existing lettuce detection model. You have the choice to train the entire model or just the final layer. Training the entire model will take longer and require more data. Training just the final layer will be faster and require less data. The final layer is the layer that makes the final prediction of lettuce. To improve the model to detect your specific lettuce, it may make more sense to fine-tune just the final few layers. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## You can skip this step if you want train the entire model\n",
    "\n",
    "# Set up existing model to train just the Region Proposal Network (RPN) and ROI Heads (last layers)\n",
    "torch_model = model.get_internal_model()\n",
    "\n",
    "for name, p in torch_model.named_parameters():\n",
    "    print(name, p.requires_grad)\n",
    "\n",
    "    if 'roi_heads' not in name and 'rpn' not in name:\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAINING RUN!!\n",
    "# initialize a model with the target labels and fit the model\n",
    "# This step will take some time and is accelerated with GPU\n",
    "losses = model.fit(train_loader,\n",
    "                   epochs=8,\n",
    "                   lr_step_size=5,\n",
    "                   learning_rate=0.001,\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the newly fine-tuned model to predict lettuce in a single image tile\n",
    "\n",
    "image = read_image('./cropped_tiles/tile_246.jpg')\n",
    "labels, boxes, scores = model.predict(image)\n",
    "show_labeled_image(image, boxes, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lettuce_detecto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
